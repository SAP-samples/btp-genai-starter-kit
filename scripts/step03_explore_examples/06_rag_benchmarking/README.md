# RAG Benchmarking examples

## Installation
1. Create virtual environment

Poetry automatically creates and manages virtual environments. To create one for your project, run:

```sh
poetry install
```

This command creates a virtual environment and installs any dependencies specified in your pyproject.toml file.


2. Run the script

```sh
poetry run python main.py
```

## Data Ingestion

Markdown files from the 'Terraform Provider for SAP BTP' git repository are read using LangChain GitLoader.
We will then create document chunks and store embedding vectors in SAP HANA Cloud Vector Engine using the LangChain Vector store adapter.

## Example: RAG Benchmarking with LLM-as-a-judge

This example demonstrates how to benchmark RAG applications using a Large Language Model (LLM) as a judge. This method does not require a predefined golden test set. The LLM directly assesses the quality of responses generated by the RAG system, providing scores or judgments.

## Example: Generate Golden Test Set with Ragas Framework

This example demonstrates how to generate a golden test set using the Ragas framework. This would help to design and produce your own golden test sets to facilitate accurate benchmarking.

## Example: RAG Benchmarking with LLM-as-a-judge using golden test set

This example covers the benchmarking process using a golden test set. A golden test set is a collection of query-response pairs that serve as a standard for evaluating the performance of the RAG application. By comparing the generated responses to the golden test set, you can objectively measure the quality and performance of the RAG system.

The benchmarking process evaluates the RAG system using the following metrics:

#### 1. Answer Correctness
- **Description**: Measures the accuracy of the generated answer when compared to the ground truth.
- **Evaluation**: This metric relies on the ground truth and the generated answer, with scores ranging from 0 to 1. A higher score indicates a closer alignment between the generated answer and the ground truth, signifying better correctness.

#### 2. Answer Relevance
- **Description**: Assesses how pertinent the generated answer is to the user question.
- **Evaluation**: A lower score is assigned to answers that are incomplete or contain redundant information, while higher scores indicate better relevancy. This metric is computed using the user input, the retrieved contexts, and the response.

#### 3. Answer Faithfulness
- **Description**: Measures the factual consistency of the generated answer against the given context.
- **Evaluation**: This metric is calculated from the answer and the retrieved context. The answer is scaled to a range of 0 to 1, with higher scores indicating better faithfulness.

#### 4. Context Relevance
- **Description**: Measures the relevancy of the retrieved context.
- **Evaluation**: This metric is calculated based on both the question and the contexts. The values fall within the range of 0 to 1, with higher values indicating better relevancy.

## Example: RAG Benchmarking with Ragas

This example demonstrates how to benchmark RAG applications using the Ragas framework. 
Ragas provides tools to evaluate Large Language Model (LLM) applications and supports several LLM based and Non-LLM based metrics. Ragas Critiques offers a range of predefined aspects like correctness, harmfulness, coherence, correctness and conciseness. Critiques within the LLM evaluators evaluate submissions based on the provided aspect.

The example uses Ragas to evaluate the RAG system using the following metrics:

#### 1. Context Precision
Measures the proportion of relevant contexts retrieved relative to the total number of contexts returned.

#### 2. Faithfulness
Measures the factual consistency of the generated answer against the given context.

#### 3. Answer Relevancy
Assesses how pertinent the generated answer is to the user question.

#### 4. Answer Correctness
Measures the accuracy of the generated answer when compared to the ground truth.

#### 5. Context Recall
Evaluates the proportion of relevant contexts retrieved out of all possible relevant contexts available.

#### 6. Answer Similarity
Assesses how similar the generated answer is to a predefined correct answer or ground truth.

#### 7. Coherence
Evaluates how logically consistent and connected the generated response is as a whole.

#### 8. Harmfulness
Measures the potential for generated content to cause harm or spread misinformation.

#### 9. Maliciousness
Assesses the likelihood that generated responses contain malicious intent or promote harmful actions.

#### 10. BLEU Score
BLEU (Bilingual Evaluation Understudy) measures the quality of generated text by comparing it to one or more reference texts.

#### 11. ROUGE Score
ROUGE (Recall-Oriented Understudy for Gisting Evaluation) measures the overlap between the generated text and reference text, focusing on recall.